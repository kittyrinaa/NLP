{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f518c4-8d81-42fe-880c-0376bcd1e4a6",
   "metadata": {},
   "source": [
    "### Task 0. Train your own doc2vec model on a test dataset. Most of the example files use Parquet file format. A short guide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc1f58ca-c13b-4fcf-a9f6-a2c13316ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.test.utils\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "827a551b-1005-41f7-87ed-57c5ffec628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['while', 'you', 're', 'enjoying', 'delicious', 'meal', 'thousands', 'of', 'disaster', 'victims', 'children', 'amp', 'refugees', 'suffer', 'from', 'hunger', 'and', 'extreme', 'climate', 'chill', 'heat', 'challenge', 'you', 'to', 'donate', 'the', 'equivalent', 'cost', 'of', 'cup', 'of', 'coffee', 'to', 'unicef', 'victims', 'chill', 'heatasg'], [2]>\n",
      "['rt', 'mmcrypto', 'usd', 'is', 'just', 'another', 'memecoin']\n"
     ]
    }
   ],
   "source": [
    "def read_corpus_from_parquet(parquet_file, text_column=\"text\", tokens_only=False):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    for i, text in enumerate(df[text_column]):\n",
    "        tokens = gensim.utils.simple_preprocess(str(text))  \n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield TaggedDocument(tokens, [i])\n",
    "\n",
    "train_parquet = 'train-DataEntity_chunk_15.parquet'\n",
    "test_parquet = 'train-DataEntity_chunk_1.parquet'\n",
    "\n",
    "train_corpus = list(read_corpus_from_parquet(train_parquet))\n",
    "test_corpus = list(read_corpus_from_parquet(test_parquet, tokens_only=True))\n",
    "\n",
    "print(train_corpus[2])\n",
    "print(test_corpus[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1e7e515-178e-45f8-b6ff-37b231e87d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e0d7395-417e-4a90-be4f-3bdbf5c8a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35744ce4-31f8-4522-a81c-28c90416a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e8677f1-a87e-415c-8ad3-75402eef6696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x186530bd9c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e88fcee7-dbf1-4101-97d8-2d1a6ababbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01817703  0.02002949  0.03789958 -0.03567206 -0.01733046 -0.14292629\n",
      "  0.15115239  0.19528143 -0.10090808 -0.11357246 -0.00113263 -0.03175327\n",
      "  0.07791322  0.08267552  0.12688895  0.03368278 -0.00714415 -0.07524869\n",
      " -0.1872876  -0.02960718 -0.01637765 -0.01885884  0.21894124 -0.07404923\n",
      "  0.02480913 -0.11801729 -0.02311789  0.05502791 -0.17846859 -0.19118147\n",
      " -0.04015056  0.04998495 -0.29385495  0.13571197 -0.01317876  0.2122886\n",
      "  0.12050559 -0.03167946 -0.02401617  0.07572369  0.15291314  0.04277322\n",
      " -0.07160015  0.07232527  0.17524458  0.02997306 -0.28075707 -0.02115787\n",
      "  0.00558015  0.06790783]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "273efad2-8017-4526-be00-3b6d8e1ba232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1214, 1: 210, 2: 97, 3: 69, 4: 42, 5: 41, 7: 26, 6: 19, 8: 12, 9: 12, 16: 10, 10: 10, 14: 9, 13: 9, 12: 8, 11: 7, 18: 6, 26: 6, 21: 6, 17: 6, 19: 5, 20: 4, 25: 4, 38: 4, 29: 3, 48: 3, 15: 3, 24: 3, 33: 3, 46: 2, 30: 2, 23: 2, 60: 2, 83: 2, 35: 2, 34: 2, 511: 2, 27: 2, 124: 2, 144: 2, 164: 2, 31: 2, 193: 2, 89: 2, 107: 2, 95: 2, 113: 2, 73: 2, 39: 2, 22: 2, 1291: 1, 905: 1, 50: 1, 68: 1, 541: 1, 1201: 1, 368: 1, 1831: 1, 98: 1, 101: 1, 533: 1, 492: 1, 1050: 1, 1316: 1, 1112: 1, 1243: 1, 147: 1, 1508: 1, 350: 1, 182: 1, 478: 1, 787: 1, 434: 1, 540: 1, 526: 1, 416: 1, 117: 1, 158: 1, 116: 1, 407: 1, 142: 1, 523: 1, 240: 1, 901: 1, 40: 1, 1293: 1, 100: 1, 1883: 1, 1929: 1, 1549: 1, 1714: 1, 1612: 1, 1574: 1, 1777: 1, 1596: 1, 1389: 1, 805: 1, 1334: 1, 296: 1, 379: 1, 503: 1, 1048: 1, 1019: 1, 213: 1, 337: 1, 572: 1, 248: 1, 70: 1, 133: 1, 197: 1, 251: 1, 361: 1, 387: 1, 104: 1, 143: 1, 224: 1, 52: 1, 28: 1, 108: 1, 935: 1, 67: 1, 37: 1, 766: 1, 314: 1, 639: 1, 1829: 1, 1790: 1, 886: 1, 65: 1, 71: 1, 32: 1, 41: 1, 1061: 1, 440: 1, 99: 1, 132: 1, 883: 1, 393: 1, 205: 1, 219: 1, 236: 1, 84: 1, 114: 1, 1024: 1, 1822: 1, 154: 1, 1551: 1, 721: 1, 995: 1, 1301: 1, 559: 1, 245: 1, 390: 1, 489: 1, 42: 1, 136: 1, 266: 1})\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8b2bec5-a3e2-4df5-87a4-003f8beca708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (16061): «seiyans unified liquidity is here openoceanglobal joins sei for the best swap prices in one place seinetwork defi»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (132, 0.7481477856636047): «irreverent vibes reserva por whatsapp laleche puertovallarta dinner food puertovallarta gastronomia fashion white juevesitos»\n",
      "\n",
      "MEDIAN (1430, 0.5219685435295105): «every outfit is an opportunity to reinvent yourself buy now log on todownload the app now designerstyles outfit fashion churchsuits»\n",
      "\n",
      "LEAST (1444, -0.42181989550590515): «ash that falls art fashion music»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50049980-0f2f-4c9d-a4bb-b7033769b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (9542): «aliceusdt losing its support for more signals link in bio crypto_news aliceusdt portalusdt fkokiusdt darusdt magicusdt bybit ethusdt btcusdt btc btc binance»\n",
      "\n",
      "Most Similar Documents:\n",
      "Document 1707 (Similarity: 0.6293): «vote for trump gt btc to gt dog to»\n",
      "\n",
      "Document 286 (Similarity: 0.5626): «join me for dice and slots with unlimited faucet grab treasure boxes to share btc bonus pool dice slots btc gaming onlinecasino crypto onlinegaming»\n",
      "\n",
      "Document 310 (Similarity: 0.5259): «join me for dice and slots with unlimited faucet grab treasure boxes thare btc bonus pool dice slots btc gaming onlinecasino crypto onlinegaming»\n",
      "\n",
      "Document 1386 (Similarity: 0.5253): «join me for dice and slots with unlimited faucet grab treasure boxes to share btc bonus pool dice slots btc gaming onlinecasino crypto onlinegaming»\n",
      "\n",
      "Document 333 (Similarity: 0.5189): «join me for dice and slots with unlimited faucet grab treasure boxes to share btc bonus pool dice slots btc gaming onlinecasino crypto onlinegaming»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random test document\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=5)\n",
    "\n",
    "print(f\"Test Document ({doc_id}): «{' '.join(test_corpus[doc_id])}»\\n\")\n",
    "\n",
    "# Display most similar documents\n",
    "print(\"Most Similar Documents:\")\n",
    "for sim_id, similarity in sims:\n",
    "    try:\n",
    "        sim_id = int(sim_id)  # Convert document ID to integer\n",
    "        if 0 <= sim_id < len(train_corpus):  \n",
    "            print(f\"Document {sim_id} (Similarity: {similarity:.4f}): «{' '.join(train_corpus[sim_id].words)}»\\n\")\n",
    "        else:\n",
    "            print(f\"Document {sim_id} is out of bounds for train_corpus.\\n\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error processing document {sim_id}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb2ba6-5db8-4b83-994d-3144ad4a0538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
