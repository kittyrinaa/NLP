{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9921b4",
   "metadata": {},
   "source": [
    "### Task 0.\n",
    "Execute the notebook and complete listed exercises (between CODE_START and CODE_END blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee131a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337c496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77ae3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9d3450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9d967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ccaaf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af353a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202715f0",
   "metadata": {},
   "source": [
    "Let’s write a function that will lemmatize twitter tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f1a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cc85b",
   "metadata": {},
   "source": [
    "Convert PoS tags into a format used by the lemmatizer using the following rules:\n",
    "\n",
    "* NN -> n\n",
    "* VB -> v\n",
    "* else -> a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32c2bc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@groovinshawn',\n",
       " 'they',\n",
       " 'be',\n",
       " 'rechargeable',\n",
       " 'and',\n",
       " 'it',\n",
       " 'normally',\n",
       " 'come',\n",
       " 'with',\n",
       " 'a',\n",
       " 'charger',\n",
       " 'when',\n",
       " 'u',\n",
       " 'buy',\n",
       " 'it',\n",
       " ':)']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    # CODE_START\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    # CODE_END\n",
    "\n",
    "    return lemmatized_sentence\n",
    "\n",
    "lemmatize_sentence(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b621a",
   "metadata": {},
   "source": [
    "Now we can proceed to processing. During processing, we will perform cleanup:\n",
    "\n",
    "remove URLs and mentions using regexes\n",
    "after lemmatization, remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4f58d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e84499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee4ebd",
   "metadata": {},
   "source": [
    "Now, please write the process_tokens() function. It should be an improved version of lemmatize_sentence() function above.\n",
    "\n",
    "It should do the following:\n",
    "\n",
    "1. Iterate through pos_tag(tweet_tokens).\n",
    "2. Use regex to remove tokens matching URLs or mentions (@somebody).\n",
    "3. Remove tokens that stop words or are punctuation symbols (use Python’s built-in string.punctuation).\n",
    "4. Lowercase all tokens\n",
    "5. Lemmatize using WordNetLemmatizer.\n",
    "6. Return the list of cleaned_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17652762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "      # CODE_START\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
    "            continue\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "      # CODE_END\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25afcf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402521b",
   "metadata": {},
   "source": [
    "Now run process_tokens on all positive/negative tokens (use tokenized method as mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "201ac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE_START\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2275d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934804b",
   "metadata": {},
   "source": [
    "Now, let’s check what words are most common.\n",
    "\n",
    "First, add a helper function get_all_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da1bafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "  # CODE_START\n",
    "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
    "  # CODE_END\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5258ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# CODE_START\n",
    "# use all_pos_words\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f249d",
   "metadata": {},
   "source": [
    "### Task 1. \n",
    "Change the code so it removes hashtags during pre-processing. (E.g. #Ukraine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55fe7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens_modify(tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token) or re.search(r'#[A-Za-z0-9_]+', token)):\n",
    "            continue\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01a74686",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens_modify(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens_modify(tokens) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30df1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf41244",
   "metadata": {},
   "source": [
    "### Task 3. \n",
    "Let’s suppose that semantic distance between words is the distance to the common semantic parent (hypernym). Write a function that will compute this distance between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8496627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def semantic_distance(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)[0]\n",
    "    synsets2 = wn.synsets(word2)[0]\n",
    "    \n",
    "    distance = synsets1.shortest_path_distance(synsets2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "print(semantic_distance(\"dog\", \"cat\"))  \n",
    "print(semantic_distance(\"car\", \"rose\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5f52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
